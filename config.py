import os
import warnings
import logging
import sys

def setup_environment():
    """
    ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì „ í™˜ê²½ ì„¤ì • ë° ê²½ê³  ì–µì œ
    """
    # TensorFlow ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ì˜¤ë¥˜ë§Œ í‘œì‹œ)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    
    # CUDA ê´€ë ¨ ê²½ê³  ì–µì œ
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # GPU ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
    
    # Transformers ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì • (ì„ íƒì‚¬í•­)
    # os.environ['TRANSFORMERS_CACHE'] = './model_cache'
    
    # Python ê²½ê³  í•„í„° ì„¤ì •
    warnings.filterwarnings('ignore', category=FutureWarning)
    warnings.filterwarnings('ignore', category=UserWarning)
    warnings.filterwarnings('ignore', category=DeprecationWarning)
    
    # ë¡œê¹… ì„¤ì •
    logging.getLogger('tensorflow').setLevel(logging.ERROR)
    logging.getLogger('transformers').setLevel(logging.ERROR)
    logging.getLogger('huggingface_hub').setLevel(logging.ERROR)
    logging.getLogger('chromadb').setLevel(logging.ERROR)
    
    # MediaPipe ë¡œê·¸ ì–µì œ
    os.environ['GLOG_minloglevel'] = '3'
    
    print("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ - ê²½ê³  ë©”ì‹œì§€ ì–µì œë¨")

def check_model_cache():
    """
    ëª¨ë¸ ìºì‹œ ìƒíƒœ í™•ì¸
    """
    cache_dirs = [
        os.path.expanduser("~/.cache/huggingface"),
        os.path.expanduser("~/.cache/torch"),
        "./model_cache"
    ]
    
    total_cache_size = 0
    for cache_dir in cache_dirs:
        if os.path.exists(cache_dir):
            for root, dirs, files in os.walk(cache_dir):
                total_cache_size += sum(os.path.getsize(os.path.join(root, file)) for file in files)
    
    if total_cache_size > 0:
        cache_size_mb = total_cache_size / (1024 * 1024)
        print(f"ğŸ“¦ ëª¨ë¸ ìºì‹œ í¬ê¸°: {cache_size_mb:.1f} MB")
    else:
        print("ğŸ“¦ ëª¨ë¸ ìºì‹œ ì—†ìŒ - ì²« ì‹¤í–‰ ì‹œ ë‹¤ìš´ë¡œë“œë©ë‹ˆë‹¤")

def print_startup_info():
    """
    ì‹œì‘ ì •ë³´ ì¶œë ¥
    """
    print("=" * 50)
    print("ğŸš€ FastAPI ì—ë”” ëŒ€í™” ì‹œì‘")
    print("=" * 50)
    
    # Python ë²„ì „ ì •ë³´
    print(f"ğŸ Python ë²„ì „: {sys.version.split()[0]}")
    
    # GPU ì •ë³´ (ì„ íƒì‚¬í•­)
    try:
        import torch
        if torch.cuda.is_available():
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    except ImportError:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    check_model_cache()
    print("-" * 50)

if __name__ == "__main__":
    setup_environment()
    print_startup_info()